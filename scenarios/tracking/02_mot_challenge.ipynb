{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Copyright (c) Microsoft Corporation. All rights reserved.</i>\n",
    "\n",
    "<i>Licensed under the MIT License.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating a Multi-Object Tracking Model on MOT Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook provides a framework for evaluating [FairMOT](https://github.com/ifzhang/FairMOT) on the [MOT Challenge dataset](https://motchallenge.net/).\n",
    "\n",
    "The MOT Challenge datasets are some of the most common benchmarking datasets for measuring multi-object tracking performance on pedestrian data. They provide distinct datasets every few years; their current offerings include MOT15, MOT16/17, and MOT 19/20. These datasets contain various annotated video sequences, each with different tracking difficulties. Additionally, MOT Challenge provides detections for tracking algorithms without detection components.\n",
    "\n",
    "The goal of this notebook is to re-produce published results on the MOT challenge using the state-of-the-art FairMOT approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure edits to libraries are loaded and plotting is shown in the notebook.\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TorchVision: 0.4.0a0+6b959ee\n",
      "Torch is using GPU: Tesla K80\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import os.path as osp\n",
    "import sys\n",
    "import time\n",
    "\n",
    "from urllib.parse import urljoin\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "from utils_cv.common.data import data_path, download, unzip_url\n",
    "from utils_cv.common.gpu import which_processor, is_windows\n",
    "from utils_cv.tracking.data import Urls\n",
    "from utils_cv.tracking.dataset import TrackingDataset\n",
    "from utils_cv.tracking.model import TrackingLearner\n",
    "\n",
    "# Change matplotlib backend so that plots are shown for windows\n",
    "if is_windows():\n",
    "    plt.switch_backend(\"TkAgg\")\n",
    "\n",
    "print(f\"TorchVision: {torchvision.__version__}\")\n",
    "which_processor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above torchvision command displays your machine's GPUs (if it has any) and the compute that `torch/torchvision` is using."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will set some model runtime parameters. Here we will specify the default FairMOT model (dla34) and will evaluate against the MOT17 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using torch device: cuda\n"
     ]
    }
   ],
   "source": [
    "CONF_THRES = 0.4\n",
    "TRACK_BUFFER = 30\n",
    "IM_SIZE = (1080, 1920)\n",
    "\n",
    "# Downloaded MOT Challendage data path\n",
    "MOT_ROOT_PATH = \"../../data/\"\n",
    "RESULT_ROOT = \"./results\"\n",
    "EXP_NAME = \"MOT_val_all_dla34\"\n",
    "\n",
    "BASELINE_MODEL = \"./models/all_dla34.pth\"\n",
    "MOTCHALLENGE_BASE_URL = \"https://motchallenge.net/data/\"\n",
    "\n",
    "# train on the GPU or on the CPU, if a GPU is not available\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(f\"Using torch device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model and Dataset Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will download the [MOT17](https://motchallenge.net/data/MOT17.zip) dataset and save it to `MOT_SAVED_PATH`. Note: MOT17 is around 5GB and it may take some time to download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data saved to ../../data/MOT17/train\n",
      "Test data saved to ../../data/MOT17/test\n"
     ]
    }
   ],
   "source": [
    "mot_path = urljoin(MOTCHALLENGE_BASE_URL, \"MOT17.zip\")\n",
    "mot_train_path = osp.join(MOT_ROOT_PATH, \"MOT17\", \"train\")\n",
    "mot_test_path = osp.join(MOT_ROOT_PATH, \"MOT17\", \"test\")\n",
    "# seqs_str:  various video sequences subfolder names under MOT challenge data\n",
    "train_seqs = [\n",
    "    \"MOT17-02-SDP\",\n",
    "    \"MOT17-04-SDP\",\n",
    "    \"MOT17-05-SDP\",\n",
    "    \"MOT17-09-SDP\",\n",
    "    \"MOT17-10-SDP\",\n",
    "    \"MOT17-11-SDP\",\n",
    "    \"MOT17-13-SDP\",\n",
    "]\n",
    "test_seqs = [\n",
    "    \"MOT17-01-SDP\",\n",
    "    \"MOT17-03-SDP\",\n",
    "    \"MOT17-06-SDP\",\n",
    "    \"MOT17-07-SDP\",\n",
    "    \"MOT17-08-SDP\",\n",
    "    \"MOT17-12-SDP\",\n",
    "    \"MOT17-14-SDP\",\n",
    "]\n",
    "\n",
    "unzip_url(mot_path, dest=MOT_ROOT_PATH, exist_ok=True)\n",
    "print(f\"Training data saved to {mot_train_path}\")\n",
    "print(f\"Test data saved to {mot_test_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pre-trained, baseline FairMOT model - `all_dla34.pth`- can be downloaded [here](https://drive.google.com/file/d/1udpOPum8fJdoEQm6n0jsIgMMViOMFinu/view). \n",
    "\n",
    "Please upload and save `all_dla34.pth` to the `BASELINE_MODEL` path."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below initializes and loads the model using the TrackingLearner class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker = TrackingLearner(None, BASELINE_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate on Training Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MOT17 provides ground truth annotations for only the training set, so we will be using the training set for evaluation.\n",
    "\n",
    "To evaluate FairMOT on this dataset, we take advantage of the [py-motmetrics](https://github.com/cheind/py-motmetrics) repository. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved tracking results to ./results/MOT_val_all_dla34/MOT17-02-SDP.txt\n",
      "Evaluate seq: MOT17-02-SDP\n",
      "Saved tracking results to ./results/MOT_val_all_dla34/MOT17-04-SDP.txt\n",
      "Evaluate seq: MOT17-04-SDP\n",
      "Saved tracking results to ./results/MOT_val_all_dla34/MOT17-05-SDP.txt\n",
      "Evaluate seq: MOT17-05-SDP\n",
      "Saved tracking results to ./results/MOT_val_all_dla34/MOT17-09-SDP.txt\n",
      "Evaluate seq: MOT17-09-SDP\n",
      "Saved tracking results to ./results/MOT_val_all_dla34/MOT17-10-SDP.txt\n",
      "Evaluate seq: MOT17-10-SDP\n",
      "Saved tracking results to ./results/MOT_val_all_dla34/MOT17-11-SDP.txt\n",
      "Evaluate seq: MOT17-11-SDP\n",
      "Saved tracking results to ./results/MOT_val_all_dla34/MOT17-13-SDP.txt\n",
      "Evaluate seq: MOT17-13-SDP\n",
      "              IDF1   IDP   IDR  Rcll  Prcn  GT  MT  PT ML    FP    FN IDs    FM  MOTA  MOTP IDt IDa IDm\n",
      "MOT17-02-SDP 60.1% 68.9% 53.3% 72.3% 93.4%  62  28  25  9   950  5148 243   628 65.9% 0.198 138  49  16\n",
      "MOT17-04-SDP 83.2% 85.1% 81.2% 86.3% 90.4%  83  51  21 11  4348  6526  32   219 77.1% 0.172   6  20   1\n",
      "MOT17-05-SDP 73.1% 75.5% 70.9% 83.9% 89.2% 133  74  50  9   702  1117 120   217 72.0% 0.221 100  45  41\n",
      "MOT17-09-SDP 65.9% 71.6% 61.0% 81.5% 95.7%  26  18   8  0   197   984  45   105 77.0% 0.175  34  10   7\n",
      "MOT17-10-SDP 63.5% 68.4% 59.2% 79.0% 91.3%  57  30  27  0   970  2700 182   439 70.0% 0.233 109  44  14\n",
      "MOT17-11-SDP 82.2% 82.0% 82.4% 92.3% 91.9%  75  55  16  4   764   722  75   112 83.5% 0.171  29  34   9\n",
      "MOT17-13-SDP 64.6% 65.9% 63.3% 77.2% 80.3% 110  50  51  9  2203  2658 164   457 56.8% 0.279  97  39  42\n",
      "OVERALL      73.9% 77.5% 70.7% 82.3% 90.1% 546 306 198 42 10134 19855 861  2177 72.5% 0.196 513 241 130\n"
     ]
    }
   ],
   "source": [
    "strsummary = tracker.eval_mot(\n",
    "    conf_thres=CONF_THRES,\n",
    "    track_buffer=TRACK_BUFFER,\n",
    "    im_size=IM_SIZE,\n",
    "    data_root=mot_train_path,\n",
    "    seqs=train_seqs,\n",
    "    result_root=RESULT_ROOT,\n",
    "    exp_name=EXP_NAME,\n",
    ")\n",
    "print(strsummary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate on Test Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For evaluating a model on the testing dataset, the MOT Challenge provides the [MOT evaluation server](https://motchallenge.net/instructions/). Here, a user can upload and submit a txt file of prediction results; the service will return metrics. After uploading our results to the [MOT17 evaluation server](https://motchallenge.net/results/MOT17/?det=Private), we can see a MOTA of 68.5 using the 'all_dla34.pth' baseline model.\n",
    "\n",
    "<img src=\"media/mot_results.PNG\" style=\"width: 737.5px;height: 365px\"/>\n",
    "\n",
    "The reported evaluation results from [FairMOT paper](https://arxiv.org/abs/2004.01888) with test set are as follows:\n",
    "\n",
    "| Dataset | MOTA   | IDF1 | IDS | MT | ML | FPS |\n",
    "|------|------|------|------|------|------|------|\n",
    "|   MOT16  | 68.7| 70.4| 953| 39.5%| 19.0%| 25.9|\n",
    "|   MOT17  | 67.5| 69.8| 2868| 37.7%| 20.8%| 25.9|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved tracking results to ./results/MOT_val_all_dla34/MOT17-01-SDP.txt\n",
      "Saved tracking results to ./results/MOT_val_all_dla34/MOT17-03-SDP.txt\n",
      "Saved tracking results to ./results/MOT_val_all_dla34/MOT17-06-SDP.txt\n",
      "Saved tracking results to ./results/MOT_val_all_dla34/MOT17-07-SDP.txt\n",
      "Saved tracking results to ./results/MOT_val_all_dla34/MOT17-08-SDP.txt\n",
      "Saved tracking results to ./results/MOT_val_all_dla34/MOT17-12-SDP.txt\n",
      "Saved tracking results to ./results/MOT_val_all_dla34/MOT17-14-SDP.txt\n"
     ]
    }
   ],
   "source": [
    "tracker.eval_mot(\n",
    "    conf_thres=CONF_THRES,\n",
    "    track_buffer=TRACK_BUFFER,\n",
    "    im_size=IM_SIZE,\n",
    "    data_root=mot_test_path,\n",
    "    seqs=test_seqs,\n",
    "    result_root=RESULT_ROOT,\n",
    "    exp_name=EXP_NAME,\n",
    "    run_eval=False,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "356.263px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
