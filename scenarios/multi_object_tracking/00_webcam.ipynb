{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Copyright (c) Microsoft Corporation. All rights reserved.</i>\n",
    "\n",
    "<i>Licensed under the MIT License.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quickstart: Web Cam Multi-Object Tracking\n",
    "\n",
    "\n",
    "Multi-Object tracking is the canonical computer vision task of associating multiple detected objects from one frame to another within a video or a sequence of images.\n",
    "\n",
    "This notebook shows a simple example of loading a pretrained tracking model for multi-object tracking from a webcam stream using the `torchvision` package. In particular, we will use FairMOT,a one-shot multi-object tracking model, which jointly detect objects and learn their re-ID features, developed by MSR Asia and others in this [repo](https://github.com/ifzhang/FairMOT).\n",
    "\n",
    "To understand the basics of Multi-Object-Tracking, please visit our #TODO [FAQ](FAQ.md).  For more details about the underlying technology of object tracking tasks, including finetuning, please see our [training introduction notebook](01_training_introduction.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisite for Webcam example \n",
    "This notebook assumes you have **a webcam** connected to your machine.  We use the `ipywebrtc` module to show the webcam widget in the notebook. Currently, the widget works on **Chrome** and **Firefox**. For more details about the widget, please visit `ipywebrtc` [github](https://github.com/maartenbreddels/ipywebrtc) or [documentation](https://ipywebrtc.readthedocs.io/en/latest/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TorchVision: 0.4.0a0+6b959ee\n",
      "Torch is using GPU: Tesla K80\n"
     ]
    }
   ],
   "source": [
    "# Regular Python libraries\n",
    "import io\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import urllib.request\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# IPython\n",
    "import scrapbook as sb\n",
    "from ipywebrtc import CameraStream, ImageRecorder, VideoStream, VideoRecorder\n",
    "from ipywidgets import HBox, Layout, widgets, Widget, VBox\n",
    "from ipywidgets import Video #KIP\n",
    "# Image\n",
    "from PIL import Image\n",
    "\n",
    "# TorchVision\n",
    "import torchvision\n",
    "from torchvision import transforms as T\n",
    "\n",
    "# KIP: utils_cv\n",
    "sys.path.append(\"../../\")\n",
    "from utils_cv.multi_object_tracking.display_with_bb import process_video, process_images\n",
    "from utils_cv.multi_object_tracking.file_format import func_extractBBoxes_fromXML, func_convertLTRB_toXYWH\n",
    "from utils_cv.multi_object_tracking.convert_seq_vid import vid_to_seq, seq_to_vid\n",
    "from utils_cv.multi_object_tracking.baseline import baseline_algorithm\n",
    "\n",
    "# utils_cv\n",
    "# sys.path.append(\"../../\")\n",
    "from utils_cv.common.data import data_path\n",
    "from utils_cv.common.gpu import which_processor, is_windows\n",
    "from utils_cv.detection.data import coco_labels\n",
    "from utils_cv.detection.model import DetectionLearner\n",
    "from utils_cv.detection.plot import PlotSettings, plot_boxes\n",
    "\n",
    "# Change matplotlib backend so that plots are shown for windows\n",
    "if is_windows():\n",
    "    plt.switch_backend('TkAgg')\n",
    "\n",
    "print(f\"TorchVision: {torchvision.__version__}\")\n",
    "which_processor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows your machine's GPUs (if it has any) and the computing device `torch/torchvision` is using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Pretrained Model\n",
    "\n",
    "We first load our  tracking model.\n",
    "#TODO add about FairMOT model architecture, detection and re-id head).\n",
    "#TODO add about dataset pretrained on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"./model/baseline_pedestrian\" #TODO: check with Casey, or tell user to download from FairMOT\n",
    "model_path = \"./model/finetuned_fridgeObjects\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Next, we pass our loaded tracking model into our TrackingLearner object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker = TrackingLearner(\n",
    "    load_model=model_path   \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Tracking\n",
    "\n",
    "## From Video File\n",
    "\n",
    "To illustrate, a simple example of tracking objects, we use a video of a person drinking. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fcf79b5e99645e28780c2c084bfab06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Video(value=b'\\x00\\x00\\x00 ftypisom\\x00\\x00\\x02\\x00isomiso2avc1mp41\\x00\\x00\\x00\\x08free\\x00RP\\xa8mdat\\x00\\x00\\…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Download an example video file\n",
    "VID_URL = \"https://cvbp.blob.core.windows.net/public/datasets/action_recognition/drinking.mp4\"\n",
    "vid_path = os.path.join(data_path(), \"example_vid.mp4\")\n",
    "urllib.request.urlretrieve(VID_URL, vid_path)\n",
    "\n",
    "from ipywidgets import Video\n",
    "video = Video.from_file(vid_path)\n",
    "video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the tracker's predict() method, we ask the tracking model to detect objects on each frame in this video, and associate them from one frame to another. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `predict()` method on each image in the sequence, we ask the model to detect how many objects and what they are on each image. The method returns annotation boxes that contains the bounding boxes around the identified objects, as well as the id number, whereby each new object acquires a new id number.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracks = tracker.predict(vid_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, frame 50, 51, 52 of the video are as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_list=[50, 51, 52]\n",
    "\n",
    "for frame_i in frame_list:\n",
    "    print(tracks[frame_i]) \n",
    "    plot_boxes(frame_number,video, tracks.get_frame(frame_i)[\"tracked_bboxes\"], plot_settings=PlotSettings(rect_color=(0, 255, 0))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now put overlay the generated tracking results onto the video. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_video_path = convert_trackingbbox_video(tracking_results, frame_rate) #TODO: ask about using ffmeg as cmd_str or other means, currently cv2\n",
    "\n",
    "video = Video.from_file(results_video_path)\n",
    "video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From WebCam Stream\n",
    "\n",
    "Now, we use a WebCam stream for object detection. We use `ipywebrtc` to start a webcam and get the video stream which is sent to the notebook's widget. Note that Jupyter widgets are quite unstable - if the widget below does not show then see the \"Troubleshooting\" section in this [FAQ](../classification/FAQ.md) for possible fixes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Webcam for object detection for reference, was not working, added the #KIP lines, might use and edit it to enable \"live\" tracking\n",
    "w_cam = CameraStream(\n",
    "    constraints={\n",
    "        'facing_mode': 'user',\n",
    "        'audio': False,\n",
    "        'video': { 'width': 200, 'height': 200 }\n",
    "    },\n",
    "    layout=Layout(width='200px')\n",
    ")\n",
    "# Image recorder for taking a snapshot\n",
    "w_imrecorder = ImageRecorder(stream=w_cam, layout=Layout(padding='0 0 0 50px'))\n",
    "# Label widget to show our object detection results\n",
    "w_im = widgets.Image(layout=Layout(width='200px'))\n",
    "output = widgets.Output() #KIP\n",
    "\n",
    "@output.capture() #KIP\n",
    "\n",
    "def detect_frame(_):\n",
    "    \"\"\" Detect objects on an image snapshot by using a pretrained model\n",
    "    \"\"\"\n",
    "    # Once capturing started, remove the capture widget since we don't need it anymore\n",
    "    if w_imrecorder.layout.display != 'none':\n",
    "        w_imrecorder.layout.display = 'none'\n",
    "        \n",
    "    try:\n",
    "        # Get the image and convert to RGB\n",
    "        im = Image.open(io.BytesIO(w_imrecorder.image.value)).convert('RGB')\n",
    "        \n",
    "        # Process the captured image\n",
    "        detections = detector.predict(im)\n",
    "        plot_boxes(im, detections[\"det_bboxes\"], plot_settings=PlotSettings(rect_color=(0, 255, 0)))\n",
    "        \n",
    "        # Convert the processed image back into the image widget for display\n",
    "        f = io.BytesIO()\n",
    "        im.save(f, format='png')\n",
    "        w_im.value = f.getvalue()\n",
    "        \n",
    "    except OSError:\n",
    "        # If im_recorder doesn't have valid image data, skip it. \n",
    "        pass\n",
    "    \n",
    "    # Taking the next snapshot programmatically\n",
    "    w_imrecorder.recording = True\n",
    "\n",
    "# Register detect_frame as a callback. Will be called whenever image.value changes. \n",
    "w_imrecorder.image.observe(detect_frame, 'value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa397e1e25f943ec9a831f69b18099f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(CameraStream(constraints={'facing_mode': 'user', 'audio': False, 'video': {'width': 200, 'heigh…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Show widgets\n",
    "HBox([w_cam, w_imrecorder, w_im, output]) #KIP\n",
    "#HBox([w_cam, w_imrecorder, w_im])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Webcam for video object tracking\n",
    "w_cam = CameraStream(\n",
    "    constraints={\n",
    "        'facing_mode': 'user',\n",
    "        'audio': False,\n",
    "        'video': { 'width': 200, 'height': 200 }\n",
    "    },\n",
    "    layout=Layout(width='200px')\n",
    ")\n",
    "# Image recorder for taking a snapshot\n",
    "w_imrecorder = VideoRecorder(stream=w_cam, layout=Layout(padding='0 0 0 50px')) #KIP\n",
    "#w_imrecorder = ImageRecorder(stream=w_cam, layout=Layout(padding='0 0 0 50px'))\n",
    "# Label widget to show our object detection results\n",
    "w_im = widgets.Image(layout=Layout(width='200px'))\n",
    "\n",
    "video_path = convert_trackingbbox_video(tracking_results, frame_rate) #TODO: ask about using ffmeg as cmd_str or other means, currently cv2\n",
    "\n",
    "video = Video.from_file(video_path)\n",
    "video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9f962d0c0a843a5b0a32eafe7859cca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(CameraStream(constraints={'facing_mode': 'user', 'audio': False, 'video': {'width': 200, 'heigh…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Show widgets\n",
    "#HBox([w_cam, w_imrecorder, w_im, output])\n",
    "HBox([w_cam, w_imrecorder, w_im])#, tracked_video])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, click the **capture button** in the widget above to start recording of the video you want using your webcam. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, run the following script to run the tracker on the recorded video. In the resulting video, bounding boxes are displayed to show the objects detected by the model, and the id number indicates the unique tracked object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the captured video\n",
    "webcam_video_path = './webcam/example_webcam.mp4'\n",
    "w_imrecorder.save(webcam_video_path)\n",
    "\n",
    "# Run inference on captured video\n",
    "tracking_results = tracker.predict(webcam_video_path)\n",
    "\n",
    "# Generate video from tracking results\n",
    "results_video_path = convert_trackingbbox_video(tracking_results, frame_rate) #TODO: ask about using ffmeg as cmd_str or other means, currently cv2\n",
    "\n",
    "video = Video.from_file(results_video_path)\n",
    "video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "In this notebook, we used a simple example to demonstrate how to use a pretrained tracking model to detect and track objects on across frames in a video sequence. The model,being trained on pedestrian dataset, is limited to only detect and track humans #TODO: check what final model . In the [training introduction notebook](01_training_introduction.ipynb), we will learn how to fine-tune a model on our own data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Stop the model and webcam\n",
    "Widget.close_all()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv",
   "language": "python",
   "name": "cv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "238.967px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
