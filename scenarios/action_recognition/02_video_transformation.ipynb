{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video Dataset Transformation  \n",
    "\n",
    "In this notebook, we show examples of video dataset transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "import os\n",
    "import time\n",
    "import decord\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch\n",
    "import torch.cuda as cuda\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import urllib.request\n",
    "import shutil\n",
    "\n",
    "from utils_cv.action_recognition.data import show_batch, VideoDataset\n",
    "from utils_cv.action_recognition.model import DEFAULT_MEAN, DEFAULT_STD\n",
    "from utils_cv.action_recognition import system_info\n",
    "from utils_cv.action_recognition.functional_video import denormalize\n",
    "from utils_cv.action_recognition.transforms_video import (\n",
    "    CenterCropVideo,    \n",
    "    NormalizeVideo,\n",
    "    RandomCropVideo,\n",
    "    RandomHorizontalFlipVideo,\n",
    "    RandomResizedCropVideo,\n",
    "    ResizeVideo,\n",
    "    ToTensorVideo,\n",
    ")\n",
    "\n",
    "system_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_clip(clip, size_factor=600):\n",
    "    \"\"\"Show frames in a clip\"\"\"\n",
    "    if isinstance(clip, torch.Tensor):\n",
    "        # Convert [C, T, H, W] tensor to [T, H, W, C] numpy array \n",
    "        clip = np.moveaxis(clip.numpy(), 0, -1)\n",
    "    \n",
    "    figsize = np.array([clip[0].shape[1]*len(clip), clip[0].shape[0]]) / size_factor\n",
    "    plt.tight_layout()\n",
    "    fig, axs = plt.subplots(1, len(clip), figsize=figsize)\n",
    "    for i, f in enumerate(clip):\n",
    "        axs[i].axis(\"off\")\n",
    "        axs[i].imshow(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare a Sample Video\n",
    "A sample video path:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://cvbp.blob.core.windows.net/public/datasets/action_recognition/drinking.mp4\"\n",
    "VIDEO_PATH = os.path.join(\"../../data/drinking.mp4\")\n",
    "# Download the file from `url` and save it locally under `file_name`:\n",
    "with urllib.request.urlopen(url) as response, open(VIDEO_PATH, 'wb') as out_file:\n",
    "    shutil.copyfileobj(response, out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_reader = decord.VideoReader(VIDEO_PATH)\n",
    "video_length = len(video_reader)\n",
    "print(\"Video length = {} frames\".format(video_length))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use three frames (the first, middle, and the last) to quickly visualize video transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip = [\n",
    "    video_reader[0].asnumpy(),\n",
    "    video_reader[video_length//2].asnumpy(),\n",
    "    video_reader[video_length-1].asnumpy(),\n",
    "]\n",
    "show_clip(clip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [T, H, W, C] numpy array to [C, T, H, W] tensor\n",
    "t_clip = ToTensorVideo()(torch.from_numpy(np.array(clip)))\n",
    "t_clip.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video Transformations\n",
    "\n",
    "Resizing with the original ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_clip(ResizeVideo(size=800)(t_clip))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_clip(ResizeVideo(size=800, keep_ratio=False)(t_clip))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Center cropping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_clip(CenterCropVideo(size=800)(t_clip))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random cropping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_crop = RandomCropVideo(size=800)\n",
    "show_clip(random_crop(t_clip))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_clip(random_crop(t_clip))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random resized cropping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_resized_crop = RandomResizedCropVideo(size=800)\n",
    "show_clip(random_resized_crop(t_clip))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_clip(random_resized_crop(t_clip))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalizing (and denormalizing to verify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_t_clip = NormalizeVideo(mean=DEFAULT_MEAN, std=DEFAULT_STD)(t_clip)\n",
    "show_clip(norm_t_clip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_clip(denormalize(norm_t_clip, mean=DEFAULT_MEAN, std=DEFAULT_STD))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Horizontal flipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_clip(RandomHorizontalFlipVideo(p=.5)(t_clip))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "r2p1d",
   "language": "python",
   "name": "r2p1d"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
