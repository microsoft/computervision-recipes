{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# R(2+1)D Model Fine-tuning on HMDB51  \n",
    "\n",
    "In this notebook, we show how to finetune the pretrained R(2+1)D model. We use [HMDB51](http://serre-lab.clps.brown.edu/resource/hmdb-a-large-human-motion-database/) human action dataset for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch\n",
    "import torch.cuda as cuda\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "\n",
    "from utils_cv.action_recognition.data import show_batch, VideoDataset\n",
    "from utils_cv.action_recognition.model import R2Plus1D \n",
    "from utils_cv.action_recognition import system_info\n",
    "\n",
    "system_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prerequisites\n",
    "* Download and extract HMDB51 videos under `./data/hmdb51/videos` directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = os.path.join(\"../../data/misc\", \"hmdb51\")\n",
    "VIDEO_DIR = os.path.join(DATA_ROOT, \"videos\")\n",
    "# This split is known as \"split1\"\n",
    "TRAIN_SPLIT = os.path.join(DATA_ROOT, \"hmdb51_vid_train_split_1.txt\")\n",
    "TEST_SPLIT = os.path.join(DATA_ROOT, \"hmdb51_vid_val_split_1.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model and Training Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8-frame or 32-frame models\n",
    "MODEL_INPUT_SIZE = 32\n",
    "# 16 for 8-frame model.\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "# Model configuration\n",
    "r2plus1d_hmdb51_cfgs = dict(\n",
    "    # HMDB51 dataset spec\n",
    "    num_classes=51,\n",
    "    video_dir=VIDEO_DIR,\n",
    "    train_split=TRAIN_SPLIT,\n",
    "    valid_split=TEST_SPLIT,\n",
    "    # Pre-trained model spec (\"Closer look\" and \"Large-scale\" papers)\n",
    "    base_model='ig65m',\n",
    "    sample_length=MODEL_INPUT_SIZE,     \n",
    "    sample_step=1,        # Frame sampling step\n",
    "    im_scale=128,         # After scaling, the frames will be cropped to (112 x 112)\n",
    "    mean=(0.43216, 0.394666, 0.37645),\n",
    "    std=(0.22803, 0.22145, 0.216989),\n",
    "    random_shift=True,\n",
    "    temporal_jitter_step=2,    # Temporal jitter step in frames (only for training set)\n",
    "    flip_ratio=0.5,\n",
    "    random_crop=True,\n",
    "    video_ext='avi',\n",
    ")\n",
    "\n",
    "# Training configuration\n",
    "train_cfgs = dict(\n",
    "    mixed_prec=False,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    grad_steps=2,\n",
    "    lr=0.001,         # 0.001 (\"Closer look\" paper, HMDB51)\n",
    "    momentum=0.95,\n",
    "    warmup_pct=0.3,  # First 30% of the steps will be used for warming-up\n",
    "    lr_decay_factor=0.001,\n",
    "    weight_decay=0.0001,\n",
    "    epochs=1, # 48,\n",
    "    model_name='hmdb51',\n",
    "    model_dir=os.path.join(\"checkpoints\", \"ig65m_kinetics\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model and Data\n",
    "\n",
    "Load R(2+1)D 34-layer model pre-trained on IG65M. There are two versions of the model: 8-frame model and 32-frame model based on the input clip length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = R2Plus1D(r2plus1d_hmdb51_cfgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three examples of training (transformed) clips. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.show_batch(num_samples=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.show_batch(which_data='valid', num_samples=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit(train_cfgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each accuracy is averaged batch-wise accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Test\n",
    "\n",
    "Reported accuracy from \"Closer look\" paper: 74.5% (clip accuracy of 66.1% on split1 based on VMZ repo)\n",
    "\n",
    "1. sample 10 clips uniformly sampled from each test video: [10 x 3 x 8 x 112 x 112]\n",
    "2. calculate clip-level accuracy: Use 10 batch and infer\n",
    "3. calculate video-level accuracy by averaging them\n",
    "4. average over the clips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Load Saved Weights if needed\n",
    "#learn.load(body_train_cfgs['model_name'] + \"_032\", body_train_cfgs['model_dir'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    num_gpus = cuda.device_count()\n",
    "    # Look for the optimal set of algorithms to use in cudnn. Use this only with fixed-size inputs.\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    num_gpus = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uniformly sample 10 clips per videos (\"Closer look\" papers)\n",
    "num_segments = 10\n",
    "test_ds = VideoDataset(\n",
    "    split_file=r2plus1d_hmdb51_cfgs['valid_split'],\n",
    "    video_dir=r2plus1d_hmdb51_cfgs['video_dir'],\n",
    "    num_segments=num_segments,\n",
    "    sample_length=r2plus1d_hmdb51_cfgs['sample_length'],\n",
    "    sample_step=1,\n",
    "    input_size=112,\n",
    "    im_scale=r2plus1d_hmdb51_cfgs['im_scale'],\n",
    "    resize_keep_ratio=True,\n",
    "    mean=r2plus1d_hmdb51_cfgs['mean'],\n",
    "    std=r2plus1d_hmdb51_cfgs['std'],\n",
    "    random_shift=False,\n",
    "    temporal_jitter=False,\n",
    "    flip_ratio=0.0,\n",
    "    random_crop=False,\n",
    "    random_crop_scales=None,\n",
    "    video_ext=r2plus1d_hmdb51_cfgs['video_ext'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"{} samples of {}\".format(len(test_ds), test_ds[0][0][0].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_batch(\n",
    "    test_ds[0][0],\n",
    "    r2plus1d_hmdb51_cfgs['sample_length'],\n",
    "    r2plus1d_hmdb51_cfgs['mean'],\n",
    "    r2plus1d_hmdb51_cfgs['std']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = learn.model\n",
    "model.to(device)\n",
    "if num_gpus > 1:\n",
    "    model = nn.DataParallel(model)\n",
    "\n",
    "model.eval()\n",
    "infer_times = []\n",
    "video_preds = []\n",
    "video_trues = []\n",
    "clip_preds = []\n",
    "clip_trues = []\n",
    "\n",
    "report_every = 100\n",
    "with torch.no_grad():\n",
    "    for i, (inputs, label) in enumerate(test_ds, start=1):\n",
    "        if i % report_every == 0:\n",
    "            print(\"{} samples have processed\".format(i))\n",
    "        \n",
    "        inputs = inputs.to(device, non_blocking=True)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        outputs = model(inputs)\n",
    "        infer_time = time.time() - start_time\n",
    "        \n",
    "        outputs = outputs.cpu().numpy()\n",
    "        \n",
    "        infer_times.append(infer_time)\n",
    "        video_preds.append(outputs.sum(axis=0).argmax())\n",
    "        video_trues.append(label)\n",
    "        clip_preds.extend(outputs.argmax(axis=1))\n",
    "        clip_trues.extend([label] * num_segments)\n",
    "        \n",
    "print(\"Done! {} samples have processed\".format(len(test_ds)))\n",
    "\n",
    "print(\"Avg. inference time per video (10 clips) =\", np.array(infer_times).mean() * 1000, \"ms\")\n",
    "print(\"Video prediction accuracy =\", accuracy_score(video_trues, video_preds))\n",
    "print(\"Clip prediction accuracy =\", accuracy_score(clip_trues, clip_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "r2p1d",
   "language": "python",
   "name": "r2p1d"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
